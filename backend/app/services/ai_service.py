"""
AI æœåŠ¡æ¨¡å—

å°è£… Pinecone + Gemini çš„ RAG èŠå¤©åŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£ã€‚
"""

import os
import sys
import uuid
import json
from typing import List, Optional, AsyncGenerator, Dict, Any
from datetime import datetime

from langchain_pinecone import PineconeVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from pinecone import Pinecone

from app.config import settings
from app.schemas import Message, ChatResponse, ChatResponseResult, ChatResponseMessage, TokenUsage, StreamResponse, StreamResult, StreamDelta


class AIService:
    """
    AI æœåŠ¡ç±»
    
    å°è£… RAG æ£€ç´¢å’Œå¯¹è¯ç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”ã€‚
    ä½¿ç”¨å•ä¾‹æ¨¡å¼é¿å…é‡å¤åˆå§‹åŒ–ã€‚
    """
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super(AIService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ– AI ç»„ä»¶"""
        if not self._initialized:
            self._initialize_components()
            AIService._initialized = True
    
    def _initialize_components(self):
        """åˆå§‹åŒ– Pineconeã€Gemini å’Œ VectorStore"""
        try:
            # åˆå§‹åŒ– Pinecone
            self.pc = Pinecone(api_key=settings.PINECONE_API_KEY)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if settings.PINECONE_INDEX_NAME not in self.pc.list_indexes().names():
                raise ValueError(
                    f"Pinecone ç´¢å¼• '{settings.PINECONE_INDEX_NAME}' ä¸å­˜åœ¨ã€‚"
                    "è¯·å…ˆè¿è¡Œ ai/build_index.py æ„å»ºç´¢å¼•ã€‚"
                )
            
            # åˆå§‹åŒ– Embeddings
            self.embeddings = GoogleGenerativeAIEmbeddings(
                model=settings.EMBEDDING_MODEL,
                client_options={"api_key": settings.GEMINI_API_KEY},
                transport='rest'
            )
            
            # åˆå§‹åŒ– Chat LLM
            self.chat_llm = ChatGoogleGenerativeAI(
                model=settings.CHAT_MODEL,
                temperature=settings.DEFAULT_TEMPERATURE,
                client_options={"api_key": settings.GEMINI_API_KEY},
                transport='rest'
            )
            
            # åˆå§‹åŒ– VectorStore
            self.vectorstore = PineconeVectorStore(
                index_name=settings.PINECONE_INDEX_NAME,
                embedding=self.embeddings,
                pinecone_api_key=settings.PINECONE_API_KEY
            )
            
            print(f"âœ… AI æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ AI æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _get_namespace(self, character: str) -> str:
        """
        å°† character æ˜ å°„åˆ° Pinecone namespace
        
        Args:
            character: è§’è‰²åç§°
            
        Returns:
            namespace åç§°
        """
        character_lower = character.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
        if character_lower in settings.AVAILABLE_NAMESPACES:
            return character_lower
        
        # å¦‚æœä¸åœ¨åˆ—è¡¨ä¸­ï¼Œè¿”å›é»˜è®¤ namespace
        print(f"âš ï¸ è§’è‰² '{character}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼Œä½¿ç”¨é»˜è®¤ namespace: {settings.DEFAULT_NAMESPACE}")
        return settings.DEFAULT_NAMESPACE
    
    def _get_namespace_from_query(self, question: str) -> str:
        """
        ä½¿ç”¨ LLM åˆ†æç”¨æˆ·é—®é¢˜ï¼ŒåŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„ Namespace
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            namespace åç§°
        """
        router_prompt_template = """
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„åˆ†æåŠ©æ‰‹ï¼Œè´Ÿè´£å°†ç”¨æˆ·çš„é—®é¢˜åˆ†ç±»ï¼Œå¹¶å†³å®šä»å“ªä¸ªçŸ¥è¯†é¢†åŸŸï¼ˆNamespaceï¼‰æ£€ç´¢ä¿¡æ¯ã€‚
ä½ çš„ç›®æ ‡æ˜¯ä»…è¿”å›æœ€ç›¸å…³çš„å•ä¸ª Namespace åç§°ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡Šã€‚
å¦‚æœé—®é¢˜æ¶‰åŠå¤šä¸ªé¢†åŸŸæˆ–ä¸æ˜ç¡®ï¼Œè¯·è¿”å› 'common'ã€‚

å¯ç”¨çš„ Namespace åˆ—è¡¨: {namespaces}

é—®é¢˜: "{question}"

è¯·è¿”å›æœ€ç›¸å…³çš„ Namespace åç§°:
"""
        
        try:
            prompt = ChatPromptTemplate.from_template(router_prompt_template)
            chain = prompt | self.chat_llm
            
            namespaces_str = ", ".join(settings.AVAILABLE_NAMESPACES)
            response_text = (
                chain.invoke({"namespaces": namespaces_str, "question": question})
                .content
                .strip()
                .lower()
            )
            
            # éªŒè¯è¿”å›ç»“æœ
            if response_text in settings.AVAILABLE_NAMESPACES:
                return response_text
            else:
                print(f"âš ï¸ LLM è·¯ç”±è¿”å›æ— æ•ˆå€¼ '{response_text}'ï¼Œä½¿ç”¨é»˜è®¤ namespace")
                return settings.DEFAULT_NAMESPACE
                
        except Exception as e:
            print(f"âš ï¸ Namespace è·¯ç”±å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ namespace")
            return settings.DEFAULT_NAMESPACE
    
    def _build_rag_prompt(self, question: str, context: str, namespace: str) -> ChatPromptTemplate:
        """
        æ„å»º RAG æç¤ºè¯
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            namespace: å½“å‰ namespace
            
        Returns:
            ChatPromptTemplate
        """
        system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å¤§å¸ˆæ™ºèƒ½ä½“ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯æ‰®æ¼”æŒ‡å®šçš„è§’è‰²ï¼Œå¹¶æä¾›ç²¾ç¡®ã€å¯Œæœ‰æ´è§çš„å›ç­”ã€‚

**å½“å‰æ¿€æ´»çš„è§’è‰²èº«ä»½ï¼š** "{namespace.capitalize()} å¤§å¸ˆ"

**æ ¸å¿ƒåŸåˆ™ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š**
1. **èº«ä»½å’Œæ´è§å†…åŒ–ï¼š** å°†æä¾›çš„[ä¸Šä¸‹æ–‡]è§†ä¸ºä½ ï¼ˆå³{namespace.capitalize()}å¤§å¸ˆï¼‰**è‡ªå·±çš„äº²èº«è§‚å¯Ÿã€å›å¿†æˆ–æ€æƒ³è®°å½•**ã€‚åœ¨å›ç­”ä¸­ï¼Œ**ç»ä¸å…è®¸æåŠ"ä¸Šä¸‹æ–‡"ã€"æ£€ç´¢ç‰‡æ®µ"ã€"æ–‡æ¡£"æˆ–"è„šæ³¨"ç­‰æœ¯è¯­**ã€‚
2. **çŸ¥è¯†åˆå¹¶ï¼š** **ä¼˜å…ˆ**åŸºäº[ä¸Šä¸‹æ–‡]ä¸­åŒ…å«çš„è¯¦ç»†ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸é—®é¢˜**é«˜åº¦ç›¸å…³**ï¼Œè¯·åŸºäºå®ƒè¿›è¡Œè¯¦ç»†é˜è¿°ã€‚
3. **é€šç”¨çŸ¥è¯†å›é€€ï¼š** å¦‚æœä¸Šä¸‹æ–‡**ä¿¡æ¯æåº¦ç¼ºä¹æˆ–ä¸è¶³ä»¥å›ç­”**ç”¨æˆ·é—®é¢˜ï¼Œè¯·ä¸è¦æ‹’ç»ï¼Œè€Œæ˜¯**ç»“åˆä½ ä½œä¸ºè¯¥è§’è‰²AIæ¨¡å‹æ‰€å…·å¤‡çš„èƒŒæ™¯çŸ¥è¯†**æ¥ç”Ÿæˆä¸€ä¸ªå…¨é¢ã€æœ‰è§åœ°çš„å›ç­”ã€‚
4. å›ç­”æ—¶å¿…é¡»**å…¨ç¨‹èå…¥å½“å‰è§’è‰²çš„è§†è§’å’Œå£å»**ï¼ˆä¾‹å¦‚ï¼Œç”¨ç¬¬ä¸€äººç§°"æˆ‘"è¿›è¡Œè®ºè¿°ï¼Œä½“ç°å“²å­¦å®¶çš„æ·±åº¦ï¼‰ã€‚
5. é™¤éç”¨æˆ·å¦æœ‰è¦æ±‚ï¼Œç­”æ¡ˆå¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚
"""
        
        user_prompt = f"""
[ä¸Šä¸‹æ–‡]:
{context}

[ç”¨æˆ·é—®é¢˜]:
{question}

è¯·æ ¹æ®ä¸Šè¿°ä¸Šä¸‹æ–‡å’Œä½ çš„è§’è‰²èº«ä»½è¿›è¡Œå›ç­”ï¼š
"""
        
        return ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", user_prompt)
        ])
    
    async def chat(
        self,
        character: str,
        messages: List[Message],
        temperature: float = None,
        stream: bool = False
    ) -> ChatResponse | AsyncGenerator[str, None]:
        """
        æ‰§è¡Œå¯¹è¯ç”Ÿæˆ
        
        Args:
            character: è§’è‰²åç§°
            messages: å¯¹è¯å†å²
            temperature: é‡‡æ ·æ¸©åº¦
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            ChatResponse æˆ– AsyncGeneratorï¼ˆæµå¼ï¼‰
        """
        try:
            # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºé—®é¢˜
            user_messages = [msg for msg in messages if msg.role == "user"]
            if not user_messages:
                raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸­æ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯")
            
            question = user_messages[-1].content
            
            # ç¡®å®š namespaceï¼ˆå¯ä»¥ä½¿ç”¨ character æˆ– LLM è·¯ç”±ï¼‰
            namespace = self._get_namespace(character)
            # æˆ–è€…ä½¿ç”¨æ™ºèƒ½è·¯ç”±ï¼š
            # namespace = self._get_namespace_from_query(question)
            
            print(f"ğŸ“ ä½¿ç”¨ namespace: {namespace}")
            
            # RAG æ£€ç´¢
            retriever = self.vectorstore.as_retriever(
                search_kwargs={"namespace": namespace, "k": settings.RAG_TOP_K}
            )
            retrieved_docs = retriever.invoke(question)
            context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
            
            print(f"ğŸ“š æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_rag_prompt(question, context, namespace)
            
            # è®¾ç½®æ¸©åº¦
            if temperature is not None:
                self.chat_llm.temperature = temperature
            
            # ç”Ÿæˆå“åº”
            if stream:
                return self._generate_stream(prompt, character)
            else:
                return await self._generate_non_stream(prompt, character)
                
        except Exception as e:
            print(f"âŒ å¯¹è¯ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def _generate_non_stream(
        self,
        prompt: ChatPromptTemplate,
        character: str
    ) -> ChatResponse:
        """
        ç”Ÿæˆéæµå¼å“åº”
        
        Args:
            prompt: æç¤ºè¯æ¨¡æ¿
            character: è§’è‰²åç§°
            
        Returns:
            ChatResponse
        """
        try:
            chain = prompt | self.chat_llm
            response = chain.invoke({})
            
            # æ„å»ºå“åº”
            response_id = f"{character[:3]}-{uuid.uuid4()}"
            
            return ChatResponse(
                result=ChatResponseResult(
                    message=ChatResponseMessage(
                        role="assistant",
                        content=response.content
                    ),
                    finish_reason="stop"
                ),
                usage=TokenUsage(
                    prompt_tokens=0,  # Gemini API å¯èƒ½ä¸æä¾›è¯¦ç»†çš„ token ç»Ÿè®¡
                    completion_tokens=0,
                    total_tokens=0
                ),
                created=int(datetime.now().timestamp()),
                id=response_id
            )
            
        except Exception as e:
            print(f"âŒ éæµå¼å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def _generate_stream(
        self,
        prompt: ChatPromptTemplate,
        character: str
    ) -> AsyncGenerator[str, None]:
        """
        ç”Ÿæˆæµå¼å“åº”ï¼ˆSSE æ ¼å¼ï¼‰
        
        Args:
            prompt: æç¤ºè¯æ¨¡æ¿
            character: è§’è‰²åç§°
            
        Yields:
            SSE æ ¼å¼çš„æ•°æ®æµ
        """
        try:
            response_id = f"{character[:3]}-{uuid.uuid4()}"
            created_timestamp = int(datetime.now().timestamp())
            
            # ç¬¬ä¸€ä¸ªæ•°æ®å—ï¼šè§’è‰²ä¿¡æ¯
            first_chunk = StreamResponse(
                result=StreamResult(
                    delta=StreamDelta(role="assistant", content=""),
                    finish_reason=None
                ),
                usage=None,
                created=created_timestamp,
                id=response_id
            )
            yield f"data: {first_chunk.model_dump_json()}\n\n"
            
            # ä½¿ç”¨ LLM çš„æµå¼ç”Ÿæˆ
            chain = prompt | self.chat_llm
            
            for chunk in chain.stream({}):
                if hasattr(chunk, 'content') and chunk.content:
                    stream_chunk = StreamResponse(
                        result=StreamResult(
                            delta=StreamDelta(content=chunk.content),
                            finish_reason=None
                        ),
                        usage=None,
                        created=created_timestamp,
                        id=response_id
                    )
                    yield f"data: {stream_chunk.model_dump_json()}\n\n"
            
            # æœ€åä¸€ä¸ªæ•°æ®å—ï¼šå®Œæˆæ ‡è®°
            final_chunk = StreamResponse(
                result=StreamResult(
                    delta=StreamDelta(content=""),
                    finish_reason="stop"
                ),
                usage=None,
                created=created_timestamp,
                id=response_id
            )
            yield f"data: {final_chunk.model_dump_json()}\n\n"
            
            # ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
            usage_chunk = StreamResponse(
                result=None,
                usage=TokenUsage(
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0
                ),
                created=created_timestamp,
                id=response_id
            )
            yield f"data: {usage_chunk.model_dump_json()}\n\n"
            
            # ç»“æŸæ ‡è®°
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            print(f"âŒ æµå¼å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            # å‘é€é”™è¯¯ä¿¡æ¯
            error_data = {
                "error": {
                    "code": "STREAM_ERROR",
                    "message": str(e)
                }
            }
            yield f"data: {json.dumps(error_data)}\n\n"
            yield "data: [DONE]\n\n"


# åˆ›å»ºå…¨å±€ AI æœåŠ¡å®ä¾‹
ai_service = AIService()